{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krasomil/CodeMonkeyApplication/blob/master/LB_score_32%F0%9F%8F%86%E2%98%A2%EF%B8%8FRadon_Milosz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RngKMAelr3-Q"
      },
      "source": [
        "<small><font color=gray>Notebook author: <a href=\"https://www.linkedin.com/in/olegmelnikov/\" target=\"_blank\">Oleg Melnikov</a> ¬©2021 onwards</font></small><hr style=\"margin:0;background-color:silver\">\n",
        "\n",
        "**[<font size=6>‚ò¢Ô∏èRadon</font>](https://www.kaggle.com/c/16oct23jh-radon/rules)**. [**Instructions**](https://colab.research.google.com/drive/1riOGrE_Fv-yfIbM5V4pgJx4DWcd92cZr#scrollTo=ITaPDPIQEgXV) for running Colabs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMxLe6lpNHJb"
      },
      "source": [
        "<small>**(Optional) CONSENT.** <mark>[ X ]</mark> We consent to sharing our Colab (after the assignment ends) with other students/instructors for educational purposes. We understand that sharing is optional and this decision will not affect our grade in any way. <font color=gray><i>(If ok with sharing your Colab for educational purposes, leave \"X\" in the check box.)</i></font></small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lUqrHtk9a031",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d2a809-8efa-42b1-81aa-9dcd2d7fa5ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive; drive.mount('/content/drive')   # OK to enable, if your kaggle.json is stored in Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHAYCFFkbGCF",
        "outputId": "9761f688-7ad6-4e4a-a0b3-b645ea733972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "- competition is now set to: 16oct23jh-radon\n",
            "100% 538k/538k [00:00<00:00, 156MB/s]\n",
            "Using competition: 16oct23jh-radon\n",
            "  teamId  teamName               submissionDate       score     \n",
            "--------  ---------------------  -------------------  --------  \n",
            "11109033  Team 5                 2023-10-27 15:06:12  23.21768  \n",
            "11120945  Team 4                 2023-10-28 02:29:14  30.99993  \n",
            "11088583  Ilian Torres           2023-10-28 15:53:07  30.99993  \n",
            "11114352  milosz.krasowski       2023-10-28 19:46:31  32.05898  \n",
            "11136754  JW_Rogers              2023-10-28 17:07:42  33.21113  \n",
            "11126746  Kate Kennelly          2023-10-28 03:33:49  33.45593  \n",
            "11121828  Ira Dhalawong          2023-10-28 19:37:25  34.79255  \n",
            "11125603  Alec Wayne             2023-10-28 14:06:53  35.63957  \n",
            "11132802  Bennett Blitz          2023-10-28 08:24:16  37.31084  \n",
            "11124289  Marko Zlatic           2023-10-28 03:14:43  40.92389  \n",
            "11123684  Claribel Gonell        2023-10-25 23:15:17  46.30486  \n",
            "11132590  Adam Parker            2023-10-26 00:34:14  46.38205  \n",
            "11103482  Brett Wolff            2023-10-27 15:54:58  49.89137  \n",
            "11133111  Nandan Joshi           2023-10-28 04:49:56  50.89302  \n",
            "11101242  Sterling Belleau       2023-10-26 16:52:35  53.83334  \n",
            "11137128  Michelle Frost         2023-10-28 18:05:17  55.97638  \n",
            "11103945  Macky Brock McWhirter  2023-10-28 05:22:44  60.29958  \n",
            "11112699  Amarilda               2023-10-26 01:53:56  62.31243  \n",
            "11131761  Chris Vaisnor          2023-10-25 21:06:26  63.51822  \n",
            "11121747  Kayla Ippongi          2023-10-28 16:28:08  66.07565  \n",
            "11127490  Derek Bowdle           2023-10-25 04:20:29  78.89385  \n",
            "10944777  ‚ò¢Ô∏èBaselineüêç            2023-09-20 18:38:17  81.17314  \n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle >> log  # upgrade kaggle package (to avoid a warning)\n",
        "!mkdir -p ~/.kaggle                               # .kaggle folder must contain kaggle.json for kaggle executable to properly authenticate you to Kaggle.com\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json >>log  # First, download kaggle.json from kaggle.com (in Account page) and place it in the root of mounted Google Drive\n",
        "!cp kaggle.json ~/.kaggle/kaggle.json > log       # Alternative location of kaggle.json (without a connection to Google Drive)\n",
        "!chmod 600 ~/.kaggle/kaggle.json                  # give only the owner full read/write access to kaggle.json\n",
        "!kaggle config set -n competition -v 16oct23jh-radon # set the competition context for the next few kaggle API calls. !kaggle config view - shows current settings\n",
        "!kaggle competitions download >> log              # download competition dataset as a zip file\n",
        "!unzip -o *.zip >> log                            # Kaggle dataset is copied as a single file and needs to be unzipped.\n",
        "!kaggle competitions leaderboard --show           # print public leaderboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7mfoxO3JKx3r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12726d0b-0f39-4f17-953f-41aabe50c62d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.49 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -U tfds-nightly tensorflow_addons tensorflow keras==2.12.0 uszipcode >> log # downgrade keras to bypass tf_utils import error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OLEcBocdkiR",
        "outputId": "991c1e1f-23f5-4fc6-c912-f3625f0c95b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.11 s, sys: 425 ms, total: 3.54 s\n",
            "Wall time: 4.05 s\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%time\n",
        "%%capture\n",
        "%reset -f\n",
        "from IPython.core.interactiveshell import InteractiveShell as IS; IS.ast_node_interactivity = \"all\"\n",
        "import numpy as np, pandas as pd, time, tensorflow_addons as tfa, tensorflow as tf, tensorflow.keras as keras, os\n",
        "from keras.layers import Flatten, Dense\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'; os.environ['TF_CUDNN_DETERMINISTIC'] = '1'; # allows seeding RNG on GPU\n",
        "ToCSV = lambda df, fname: df.round(2).to_csv(f'{fname}.csv', index_label='id') # rounds values to 2 decimals\n",
        "\n",
        "class Timer():\n",
        "  def __init__(self, lim:'RunTimeLimit'=60): self.t0, self.lim, _ = time.time(), lim, print(f'‚è≥ started. You have {lim} sec. Good luck!')\n",
        "  def ShowTime(self):\n",
        "    msg = f'Runtime is {time.time()-self.t0:.0f} sec'\n",
        "    print(f'\\033[91m\\033[1m' + msg + f' > {self.lim} sec limit!!!\\033[0m' if (time.time()-self.t0-1) > self.lim else msg)\n",
        "\n",
        "np.set_printoptions(linewidth=100, precision=2, edgeitems=2, suppress=True)\n",
        "pd.set_option('display.max_columns', 20, 'display.precision', 2, 'display.max_rows', 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "MGUUojSZchvX",
        "outputId": "7ec58204-787c-45af-8ddf-4f4e498be2aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Uppm   adjwt basement  cntyfips     county  dupflag  floor    lat  \\\n",
              "0      1.80   54.97        Y        59     MORTON        0      1  46.66   \n",
              "1      1.65  499.34        N        85  KOSCIUSKO        0      1  40.85   \n",
              "...     ...     ...      ...       ...        ...      ...    ...    ...   \n",
              "12571  0.44  394.07        Y         3      ANOKA        0      0  44.91   \n",
              "12572  2.71  157.82        Y        15     MOHAVE        0      0  36.01   \n",
              "\n",
              "          lon  pcterr  ...  stfips  stopdt  stoptm  stratum  typebldg wave  \\\n",
              "0     -101.39    7.76  ...      38   32206    1230        2         2    1   \n",
              "1      -86.22   55.02  ...      18   11497    1430        3         1   32   \n",
              "...       ...     ...  ...     ...     ...     ...      ...       ...  ...   \n",
              "12571  -92.86    9.40  ...      27   32110    1500        2         1    4   \n",
              "12572 -113.21   14.46  ...       4   11496    1330        1         1   38   \n",
              "\n",
              "      windoor    zip  zipflag    Y  \n",
              "0         NaN  58554        0  NaN  \n",
              "1         NaN  46580        0  NaN  \n",
              "...       ...    ...      ...  ...  \n",
              "12571     NaN  55303        0  8.6  \n",
              "12572     NaN  86403        0  1.9  \n",
              "\n",
              "[12573 rows x 27 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-30b15e88-4984-4c49-abcc-968c27635ec8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Uppm</th>\n",
              "      <th>adjwt</th>\n",
              "      <th>basement</th>\n",
              "      <th>cntyfips</th>\n",
              "      <th>county</th>\n",
              "      <th>dupflag</th>\n",
              "      <th>floor</th>\n",
              "      <th>lat</th>\n",
              "      <th>lon</th>\n",
              "      <th>pcterr</th>\n",
              "      <th>...</th>\n",
              "      <th>stfips</th>\n",
              "      <th>stopdt</th>\n",
              "      <th>stoptm</th>\n",
              "      <th>stratum</th>\n",
              "      <th>typebldg</th>\n",
              "      <th>wave</th>\n",
              "      <th>windoor</th>\n",
              "      <th>zip</th>\n",
              "      <th>zipflag</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.80</td>\n",
              "      <td>54.97</td>\n",
              "      <td>Y</td>\n",
              "      <td>59</td>\n",
              "      <td>MORTON</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>46.66</td>\n",
              "      <td>-101.39</td>\n",
              "      <td>7.76</td>\n",
              "      <td>...</td>\n",
              "      <td>38</td>\n",
              "      <td>32206</td>\n",
              "      <td>1230</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>58554</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.65</td>\n",
              "      <td>499.34</td>\n",
              "      <td>N</td>\n",
              "      <td>85</td>\n",
              "      <td>KOSCIUSKO</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>40.85</td>\n",
              "      <td>-86.22</td>\n",
              "      <td>55.02</td>\n",
              "      <td>...</td>\n",
              "      <td>18</td>\n",
              "      <td>11497</td>\n",
              "      <td>1430</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>NaN</td>\n",
              "      <td>46580</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12571</th>\n",
              "      <td>0.44</td>\n",
              "      <td>394.07</td>\n",
              "      <td>Y</td>\n",
              "      <td>3</td>\n",
              "      <td>ANOKA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>44.91</td>\n",
              "      <td>-92.86</td>\n",
              "      <td>9.40</td>\n",
              "      <td>...</td>\n",
              "      <td>27</td>\n",
              "      <td>32110</td>\n",
              "      <td>1500</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>55303</td>\n",
              "      <td>0</td>\n",
              "      <td>8.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12572</th>\n",
              "      <td>2.71</td>\n",
              "      <td>157.82</td>\n",
              "      <td>Y</td>\n",
              "      <td>15</td>\n",
              "      <td>MOHAVE</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>36.01</td>\n",
              "      <td>-113.21</td>\n",
              "      <td>14.46</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>11496</td>\n",
              "      <td>1330</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>NaN</td>\n",
              "      <td>86403</td>\n",
              "      <td>0</td>\n",
              "      <td>1.9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12573 rows √ó 27 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-30b15e88-4984-4c49-abcc-968c27635ec8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-30b15e88-4984-4c49-abcc-968c27635ec8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-30b15e88-4984-4c49-abcc-968c27635ec8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e6bd252e-e11f-4540-a947-ff7b91d3c403\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e6bd252e-e11f-4540-a947-ff7b91d3c403')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e6bd252e-e11f-4540-a947-ff7b91d3c403 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df_raw = pd.read_csv('XY_radon.csv'); df_raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ah9dc6TRfSuL",
        "outputId": "c54af54a-e970-4ecf-d798-5ed328f563d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ started. You have 60 sec. Good luck!\n"
          ]
        }
      ],
      "source": [
        "tmr = Timer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NcTKbw3KhAn"
      },
      "source": [
        "<hr color=green size=40>\n",
        "\n",
        "<strong><font color=green size=5>‚è≥Timed Green Playground (TGP): Your ideas, code, documentation, and timer START HERE!</font></strong>\n",
        "\n",
        "<font color=green>Students: Keep all your definitions, code, documentation in <b>TGP</b>. Modifying any code outside of TGP incurs penalties."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import packages\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.decomposition import PCA\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras.regularizers import l2, l1\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "import keras\n",
        "from keras import backend as K\n",
        "import random\n",
        "import os\n",
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "XJ6S6G1PX1c-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ensure repeatability\n",
        "np.random.seed(13)\n",
        "keras.utils.set_random_seed(13)\n",
        "tf.random.set_seed(0)\n",
        "# tf.config.experimental.enable_op_determinism()"
      ],
      "metadata": {
        "id": "Ufkh4xRkX2mG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJs0jS4fIO1j"
      },
      "source": [
        "<font color=green><h3><b>$\\alpha$. Split observations into train and test sets</b><h3>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# url = 'https://www.epa.gov/sites/default/files/2020-03/radon_zones-spreadsheet.xls'\n",
        "# storage_options = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3\"}\n",
        "# zones = pd.read_excel(url, storage_options=storage_options)\n",
        "\n",
        "# zones['County,State'] = zones['County,State'].apply(lambda x: str(x).upper().replace('.', '').replace(' ', '').replace('STCROIX','ST.CROIX').replace('SAGINAW', 'SAGINAWCHIPPEWA'))\n",
        "\n",
        "# zones = zones[['County,State', 'Zone']].set_index(zones['County,State']).drop(['County,State'], axis=1).dropna().to_dict()\n",
        "\n",
        "# df_raw['zone'] = df_raw.apply(lambda x: zones['Zone'][str(x['county']) + \",\" + str(x['state2'])] if (str(x['county']) + \",\" + str(x['state2']) in zones['Zone'].keys()) else np.nan, axis=1)\n",
        "\n",
        "# #Replace 3 with 1. Because 3 is the lowest radiation level in the original data\n",
        "# df_raw.loc[df_raw['zone']==3, 'zone'] = 93\n",
        "# df_raw.loc[df_raw['zone']==1, 'zone'] = 3\n",
        "# df_raw.loc[df_raw['zone']==93, 'zone'] = 1"
      ],
      "metadata": {
        "id": "5DhOkylkAtXQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Haversine distance formula can be found here: https://www.geeksforgeeks.org/haversine-formula-to-find-distance-between-two-points-on-a-sphere/#\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "\n",
        "    dLat = (lat2 - lat1) * np.pi / 180.0\n",
        "    dLon = (lon2 - lon1) * np.pi / 180.0\n",
        "\n",
        "    lat1 = lat1 * np.pi / 180.0\n",
        "    lat2 = lat2 * np.pi / 180.0\n",
        "\n",
        "    a = (np.sin(dLat / 2) ** 2 + np.sin(dLon / 2) ** 2 * np.cos(lat1) * np.cos(lat2))\n",
        "\n",
        "    rad = 6371  # radius of the Earth\n",
        "\n",
        "    c = 2 * np.arcsin(np.sqrt(a))\n",
        "\n",
        "    return rad * c"
      ],
      "metadata": {
        "id": "R27BHxYZabJm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# link to counties csv with coordinates\n",
        "zip_file = 'https://simplemaps.com/static/data/us-counties/1.73/basic/simplemaps_uscounties_basicv1.73.zip'\n",
        "# below code found at: https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url\n",
        "import zipfile, requests, io\n",
        "\n",
        "r = requests.get(zip_file)\n",
        "with zipfile.ZipFile(io.BytesIO(r.content)) as zip_ref:\n",
        "    zip_ref.extractall(os.getcwd())\n"
      ],
      "metadata": {
        "id": "C3LKJ0-1aJWH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "county_coords = pd.read_csv(os.path.join(os.getcwd(), 'uscounties.csv'))[['county', 'state_id', 'lat', 'lng']]\n",
        "county_coords['join_key'] = county_coords[['county', 'state_id']].apply(lambda row: row['county'] + ', ' + row['state_id'], axis=1)\n",
        "coord_county_names = county_coords['join_key'].values.tolist()"
      ],
      "metadata": {
        "id": "FPP0Z0ndaPww"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://www.epa.gov/sites/default/files/2020-03/radon_zones-spreadsheet.xls'\n",
        "storage_options = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3\"}\n",
        "zones = pd.read_excel(url, storage_options=storage_options).query('`COUNTY LABEL`!= \"no data\"')\n",
        "\n",
        "radon_counties = zones['County,State'].values.tolist()\n",
        "df_raw['County,State'] = df_raw[['county', 'state']].apply(lambda row: str(row['county']).title() + ', ' + row['state'], axis=1)\n"
      ],
      "metadata": {
        "id": "O84ktCwDaS4f"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_raw['distance_to_zone'] = df_raw[['County,State', 'lat', 'lon']].apply(lambda row: haversine(county_coords[county_coords['join_key'] == row['County,State']]['lat'].values.tolist()[0], county_coords[county_coords['join_key'] == row['County,State']]['lng'].values.tolist()[0], row['lat'], row['lon']) if row['County,State'] in coord_county_names and row['County,State'] in radon_counties else np.nan, axis=1)"
      ],
      "metadata": {
        "id": "lU7ZZVLpaYRo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an array of coordinates for 'County,State' from county_coords\n",
        "county_coords_dict = dict(zip(county_coords['join_key'], zip(county_coords['lat'], county_coords['lng'])))\n",
        "county_coords_array = np.array([county_coords_dict.get(county_state, (np.nan, np.nan)) for county_state in df_raw['County,State']])\n",
        "\n",
        "# Calculate haversine distances for all rows\n",
        "haversine_distances = np.array([haversine(lat1, lon1, lat2, lon2) for lat1, lon1, (lat2, lon2) in zip(county_coords_array[:,0], county_coords_array[:,1], zip(df_raw['lat'], df_raw['lon']))])\n",
        "\n",
        "# Apply conditions and assign distances or NaN\n",
        "mask = np.isin(df_raw['County,State'], coord_county_names) & np.isin(df_raw['County,State'], radon_counties)\n",
        "df_raw['distance_to_zone'] = np.where(mask, haversine_distances, np.nan)"
      ],
      "metadata": {
        "id": "vKeK8CNiXTLK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# kmeans = KMeans(n_clusters=8, n_init='auto', random_state=0)\n",
        "# resulting_clusters = kmeans.fit(df_raw[['lon', 'lat']])\n",
        "# cluster_centers = pd.DataFrame(resulting_clusters.cluster_centers_, columns=['lon', 'lat']).reset_index(names='cluster_id')\n",
        "\n",
        "# # calculate cluster ids\n",
        "# cluster_ids = pd.DataFrame(resulting_clusters.labels_, columns=['cluster_id']).reset_index(names='join_field')\n",
        "# df_raw = df_raw.reset_index(names='join_field').merge(cluster_ids, on='join_field').drop(columns='join_field', axis=1)\n",
        "\n",
        "# # calculate distance from center cluster and weighted inverse\n",
        "# df_raw['cluster_center_distance'] = df_raw[['cluster_id', 'lon', 'lat']].apply(lambda row: haversine(cluster_centers[cluster_centers['cluster_id'] == row['cluster_id']]['lat'].values.tolist()[0], cluster_centers[cluster_centers['cluster_id'] == row['cluster_id']]['lon'].values.tolist()[0], row['lat'], row['lon']), axis=1)\n",
        "# df_raw['cluster_center_weighted_inverse'] = 1 / df_raw['cluster_center_distance']\n",
        "\n",
        "# # calculate distance from center point and weighted inverse\n",
        "# median_lon, median_lat = np.median(df_raw['lon']), np.median(df_raw['lat'])\n",
        "# df_raw['dist_from_median_center'] = df_raw[['lon', 'lat']].apply(lambda row: haversine(median_lat, median_lon, row['lat'], row['lon']), axis=1)\n",
        "# df_raw['median_center_weighted_inverse'] = 1 / df_raw['dist_from_median_center']"
      ],
      "metadata": {
        "id": "uU6IJf6ra48e"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build a pipeline\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, PolynomialFeatures, PowerTransformer, OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "\n",
        "num_feat = ['Uppm','adjwt','cntyfips','dupflag','floor', 'lat', 'lon', 'pcterr','region', 'rep', 'room', 'startdt', 'starttm','stfips', 'stopdt',\n",
        "       'stoptm', 'stratum', 'typebldg','wave','distance_to_zone']#,'zone' ,'cluster_center_distance','cluster_center_weighted_inverse','dist_from_median_center','median_center_weighted_inverse'\n",
        "hotencode_feat = ['state2','basement','zip'] #, 'cluster_id'\n",
        "\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "    ('scaler', PowerTransformer())\n",
        "    ])\n",
        "# categories = [['46','85','63','58','64','18','23','17','14','56','19','54','65','55','47','15','48','86','20','11','16','27','21','26','49','12','10','25','13','24','59','70']]\n",
        "onehot_pipeline = Pipeline([\n",
        "    (\"encoder\", OneHotEncoder(sparse_output=False,  handle_unknown='infrequent_if_exist'))\n",
        "    ])\n",
        "dataset_transformer = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('scaler', num_pipeline, num_feat),\n",
        "        ('onehot', onehot_pipeline, hotencode_feat),\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "pipe = Pipeline([\n",
        "                  ('transformer',dataset_transformer),\n",
        "                  ('imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
        "                  # ('scaler', StandardScaler()),\n",
        "                  # ('polynomial', PolynomialFeatures(2)),\n",
        "                  # ('pca', PCA(250)),\n",
        "                ])\n",
        "\n",
        "# tX_transformed = pipe.fit_transform(tX)\n",
        "# vX_transformed = pipe.fit_transform(vX)"
      ],
      "metadata": {
        "id": "WilcGzNJGx0I"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Uu_dha6Kc20u"
      },
      "outputs": [],
      "source": [
        "#Drop the unnecessary data\n",
        "#drop county because it contains missing values and is approximated by cntyfips\n",
        "#drop state because state2 contains better data. state has a weird value 'R5' which doesn't exist in state2\n",
        "#drop zipflag. It's not needed\n",
        "df = df_raw.drop(columns=['windoor','county', 'state', 'zipflag','County,State'], axis=1)\n",
        "#Convert 'N' to 0 and 'Y' to 1 for basement\n",
        "df.loc[df['basement']== 'N', 'basement'] = 0\n",
        "df.loc[df['basement']== 'Y', 'basement'] = 1\n",
        "df['basement'] = df['basement'].apply(str)\n",
        "#Replace one zip which equals -1 with 58646. I checked the zipcode in google maps by inserting the latitude and longitude for that record\n",
        "df.loc[df['zip']==-1, 'zip'] = 58646\n",
        "#Keep only 2 digits from the zip code\n",
        "df['zip'] = df['zip'].apply(str)\n",
        "df['zip'] = df['zip'].str[:2]\n",
        "df['zip'] = df['zip'].apply(str)\n",
        "#split into tX and tY\n",
        "tX, tY = df.drop('Y', axis=1), df.Y\n",
        "#transform data with the pipeline\n",
        "tX_transformed = pd.DataFrame(pipe.fit_transform(tX))\n",
        "df = pd.concat([tX_transformed, tY], axis=1)\n",
        "#Split into train and validation datasets\n",
        "vX_transformed = df.query('Y!=Y').drop('Y', axis=1)    # slice a test sample\n",
        "tXY = df.query('Y==Y')                     # slice training sample\n",
        "tX_transformed, tY = tXY.drop('Y', axis=1), tXY.Y.astype(int)      # split into training I/O\n",
        "# print(tY.tolist()[:50])                    # train outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-3O279GpNPr"
      },
      "source": [
        "<font color=green><h3><b>$\\beta$. Build and train a model</b><h3>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trainX, testX, trainY, testY = train_test_split(tX, tY, train_size=0.7, test_size=0.3, shuffle = True, random_state=13)"
      ],
      "metadata": {
        "id": "3QpeVmCkCe21"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #TUNING\n",
        "# !pip install keras-tuner --upgrade\n",
        "# import keras_tuner\n",
        "\n",
        "# batch_size = 50\n",
        "\n",
        "# # Reduce learning rate on plateau\n",
        "# callbacks = [\n",
        "#     # tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True, verbose=0),\n",
        "#     tf.keras.callbacks.ReduceLROnPlateau(patience=3, factor=0.3, verbose=0)\n",
        "# ]\n",
        "\n",
        "\n",
        "# # number_hidden_layers_options = [1,2,3,4]\n",
        "# # neurons_per_layer_options = np.arange(1, 1000, step = 10)\n",
        "# # learning_rate_options = [x for x in np.geomspace(0.1, 1e-7, num=7)]\n",
        "# # dropout_rate_options = [0.0, 0.1, 0.15, 0.2]\n",
        "# # l2_weight_options = [0.0, 0.001, 0.0001, 0.00001, 0.000001]\n",
        "# # activation_func_options = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear', 'elu', 'selu', tf.keras.layers.LeakyReLU(alpha=0.05)]\n",
        "# # optimization_func_options =  ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
        "# # init_func_options = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform',keras.initializers.RandomNormal(seed=0)]\n",
        "\n",
        "# def build_model(hp):\n",
        "#     # Tune whether to add Dropout layers\n",
        "#     # hp_add_dropout = hp.Choice('add_dropout', values=[True, False])\n",
        "\n",
        "#     # Tune whether to add BatchNormalization layers\n",
        "#     # hp_add_bn = hp.Choice('add_bn', values=[True, False])\n",
        "\n",
        "#     model = keras.Sequential()\n",
        "#     model.add(keras.layers.Flatten(input_shape=[tX_transformed.shape[1]]))\n",
        "\n",
        "#     # if hp_add_dropout:\n",
        "#     #     # Tune the dropout rate from [0.1, 0.2, 0.3]\n",
        "#     #     model.add(keras.layers.Dropout(rate=hp.Choice('dropout_rate_0', values=[0.1, 0.2, 0.0,0.3], default=0.1)))\n",
        "\n",
        "#     # if hp_add_bn:\n",
        "#     #     model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "#     # Tune the L1 and L2 rates for the regularizer from [0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
        "#     hp_l1_lr = hp.Choice('l1_lr', values=[0.0, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0], default=1e-6) #0.0#\n",
        "#     hp_l2_lr = hp.Choice('l2_lr', values=[0.0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0], default=1e-5) #1e-05 #\n",
        "#     hp_bias_lr = hp.Choice('bias_lr', values=[0.0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0], default=1e-4) #0.0001 #\n",
        "\n",
        "#     # Tune activation function + initializer\n",
        "#     a_act_opts = {\n",
        "#               0: ('softmax'),\n",
        "#               1: ('softplus'),\n",
        "#               2: ('softsign'),\n",
        "#               3: ('relu'),\n",
        "#               4: ('tanh'),\n",
        "#               5: ('sigmoid'),\n",
        "#               6: ('hard_sigmoid'),\n",
        "#               7: ('linear'),\n",
        "#               8: ('elu'),\n",
        "#               9: ('selu'),\n",
        "#               10: (tf.keras.layers.LeakyReLU(alpha=0.05)),\n",
        "#               11: ('gelu'),\n",
        "#             }\n",
        "#     b_init_options = {\n",
        "#               # 0: ('uniform'),\n",
        "#               # 1: ('lecun_uniform'),\n",
        "#               # 2: ('normal'),\n",
        "#               # 3: ('zero'),\n",
        "#               # 4: ('glorot_normal'),\n",
        "#               # 5: ('glorot_uniform'),\n",
        "#               # 6: ('he_normal'),\n",
        "#               0: (tf.keras.initializers.HeNormal(seed=13)),\n",
        "#               1: (keras.initializers.RandomNormal(seed=0))\n",
        "#             }\n",
        "#     hp_a = hp.Int('activation', 0, 10, default=10)\n",
        "#     hp_b = hp.Int('init', 0, 1, default=0)\n",
        "#     n_layers = hp.Int('num_layers', 1, 6, default=3)\n",
        "#     # Tune the number of layers\n",
        "#     for i in range(n_layers):\n",
        "\n",
        "#         # Tune the number of neurons per layer + regularizers\n",
        "#         model.add(keras.layers.Dense(units=hp.Int(f\"units_{i}\",\n",
        "#                                                   min_value=10,\n",
        "#                                                   max_value=125,\n",
        "#                                                   step=1,\n",
        "#                                                   default=80),\n",
        "#                                activation=a_act_opts[hp_a],\n",
        "#                                kernel_initializer=b_init_options[hp_b],\n",
        "#                                kernel_regularizer=tf.keras.regularizers.L1L2(l1=hp_l1_lr, l2=hp_l2_lr),\n",
        "#                                bias_regularizer=tf.keras.regularizers.L2(l2=hp_bias_lr),)\n",
        "#                  )\n",
        "\n",
        "#         # if hp_add_dropout:\n",
        "#         #     # Tune the dropout rate from [0.1, 0.2, 0.3]\n",
        "#         #     model.add(keras.layers.Dropout(rate=hp.Choice(f\"dropout_rate_{i + 1}\", values=[0.1, 0.2, 0.3, 0.4], default=0.1)))\n",
        "\n",
        "#         # if hp_add_bn:\n",
        "#         #     model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "#     model.add(keras.layers.Dense(1, kernel_initializer=b_init_options[hp_b], name='output'))\n",
        "\n",
        "#     # Tune the learning rate for the optimizer from [0.01, 0.001, or 0.0001]\n",
        "#     # hp_learning_rate = hp.Choice('learning_rate', values=[x for x in np.geomspace(0.1, 1e-7, num=7)], default=0.01)\n",
        "#     hp_learning_rate = hp.Choice('learning_rate', values=[0.05,0.02,0.01,0.005])\n",
        "#     # Tune optimizer beta 1 and beta 2 from [0.7, 0.9, 0.99, 0.999]\n",
        "#     hp_beta1 = hp.Choice('beta_1', values=[0.7, 0.9, 0.99, 0.999]) #0.9 #\n",
        "#     hp_beta2 = hp.Choice('beta_2', values=[0.7, 0.9, 0.99, 0.999]) # 0.9 #\n",
        "#     # LRSchedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=hp_learning_rate, decay_steps=10000, decay_rate=0.9)\n",
        "#     c_opt_options = {\n",
        "#               0: (keras.optimizers.Nadam(learning_rate= hp_learning_rate, beta_1=hp_beta1, beta_2=hp_beta2)),\n",
        "#               1: (keras.optimizers.SGD(learning_rate=hp_learning_rate)),\n",
        "#               2: (keras.optimizers.RMSprop(learning_rate=hp_learning_rate)),\n",
        "#               3: (keras.optimizers.Adagrad(learning_rate=hp_learning_rate)),\n",
        "#               4: (keras.optimizers.Adadelta(learning_rate=hp_learning_rate)),\n",
        "#               5: (keras.optimizers.Adam(learning_rate=hp_learning_rate)),\n",
        "#               6: (keras.optimizers.Adamax(learning_rate=hp_learning_rate))\n",
        "#             }\n",
        "#     hp_c = hp.Int('optimizer', 0, 6, default=0)\n",
        "#     model.compile(optimizer=c_opt_options[hp_c],\n",
        "#                   loss='mse',\n",
        "#                   metrics=['mse'])\n",
        "#     return model\n",
        "\n",
        "# # Exponential Decay for learning rate\n",
        "\n",
        "# tuner = keras_tuner.RandomSearch(\n",
        "#     build_model,\n",
        "#     objective= keras_tuner.Objective(\"val_mse\", direction=\"min\"),\n",
        "#     max_trials=50,  # The maximum number of hyperparameter combinations to try\n",
        "#     executions_per_trial=1, # The number of times to train the model for each combination of hyperparameters\n",
        "#     # directory='dnn', # The directory where to save the results of the search\n",
        "#     project_name='trial1', # name the directory where to save the results.\n",
        "# )\n",
        "\n",
        "# tuner.search(tX_transformed, tY, epochs=30, validation_split=0.3,\n",
        "#              batch_size=batch_size,\n",
        "#              callbacks=callbacks,\n",
        "#             )\n",
        "\n",
        "# best_model = tuner.get_best_models()[0]"
      ],
      "metadata": {
        "id": "bCJJjTEwX8Hw"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best_model.summary()"
      ],
      "metadata": {
        "id": "25aQZihs2GHI"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tuner.results_summary()"
      ],
      "metadata": {
        "id": "J0Wr_U8D2LQ8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#THIS IDEA WAS USED BY TEAM 7 IN ASSIGNMENT 5. KUDOS TO TEAM 7.\n",
        "# Workaround to try to immediately return when all training epochs complete\n",
        "# Otherwise we'd see delays of 20s to a minute after training\n",
        "\n",
        "class TrainingComplete(Callback):\n",
        "    def on_train_end(self, logs=None):\n",
        "        print(\"Training has ended.\")\n",
        "        self.end_time = time.time()\n",
        "        raise('End quick')"
      ],
      "metadata": {
        "id": "iusfMD1n7iwr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters:\n",
        "# l1_lr: 1e-05\n",
        "# l2_lr: 1e-05\n",
        "# bias_lr: 0.1\n",
        "# activation: 2\n",
        "# init: 0\n",
        "# num_layers: 6\n",
        "# units_0: 111\n",
        "# units_1: 98\n",
        "# units_2: 65\n",
        "# learning_rate: 0.05\n",
        "# beta_1: 0.7\n",
        "# beta_2: 0.99\n",
        "# optimizer: 6\n",
        "# units_3: 26\n",
        "# units_4: 111\n",
        "# units_5: 91\n",
        "# Score: 20.706668853759766\n",
        "\n",
        "#     c_opt_options = {\n",
        "#               0: (keras.optimizers.Nadam(learning_rate= hp_learning_rate, beta_1=hp_beta1, beta_2=hp_beta2)),\n",
        "#               1: (keras.optimizers.SGD(learning_rate=hp_learning_rate)),\n",
        "#               2: (keras.optimizers.RMSprop(learning_rate=hp_learning_rate)),\n",
        "#               3: (keras.optimizers.Adagrad(learning_rate=hp_learning_rate)),\n",
        "#               4: (keras.optimizers.Adadelta(learning_rate=hp_learning_rate)),\n",
        "#               5: (keras.optimizers.Adam(learning_rate=hp_learning_rate)),\n",
        "#               6: (keras.optimizers.Adamax(learning_rate=hp_learning_rate))\n",
        "\n",
        "#    a_act_opts = {\n",
        "#               0: ('softmax'),\n",
        "#               1: ('softplus'),\n",
        "#               2: ('softsign'),\n",
        "#               3: ('relu'),\n",
        "#               4: ('tanh'),\n",
        "#               5: ('sigmoid'),\n",
        "#               6: ('hard_sigmoid'),\n",
        "#               7: ('linear'),\n",
        "#               8: ('elu'),\n",
        "#               9: ('selu'),\n",
        "#               10: (tf.keras.layers.LeakyReLU(alpha=0.05))\n",
        "#             }\n",
        "#     b_init_options = {\n",
        "#               0: ('uniform'),\n",
        "#               1: ('lecun_uniform'),\n",
        "#               2: ('normal'),\n",
        "#               3: ('zero'),\n",
        "#               4: ('glorot_normal'),\n",
        "#               5: ('glorot_uniform'),\n",
        "#               6: ('he_normal'),\n",
        "#               7: ('he_uniform'),\n",
        "#               8: (keras.initializers.RandomNormal(seed=0))\n",
        "\n",
        "\n",
        "\n",
        "# Init = keras.initializers.RandomNormal(seed=13)\n",
        "Init = tf.keras.initializers.HeNormal(seed=13)\n",
        "activation = 'softsign'\n",
        "hp_learning_rate = 0.01\n",
        "hp_beta1 = 0.7\n",
        "hp_beta2 = 0.99\n",
        "hp_l1_lr=  1e-05\n",
        "hp_l2_lr= 1e-05\n",
        "hp_bias_lr = 0.1\n",
        "leakyReluAlpha = 0.05\n",
        "LRSchedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=hp_learning_rate, decay_steps=10000, decay_rate=0.90)\n",
        "opt = keras.optimizers.Adamax(learning_rate=hp_learning_rate)\n",
        "\n",
        "m = keras.Sequential()\n",
        "m.add(keras.layers.Flatten(input_shape=[tX_transformed.shape[1]]))\n",
        "# m.add(keras.layers.Dropout(0))\n",
        "# earlyStopping = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True, verbose=0),\n",
        "# reducedLR = tf.keras.callbacks.ReduceLROnPlateau(patience=3, factor=0.4, verbose=0)\n",
        "\n",
        "#Hidden1\n",
        "m.add(keras.layers.Dense(units=111,\n",
        "                        activation=activation,\n",
        "                        kernel_initializer=Init,\n",
        "                        kernel_regularizer=tf.keras.regularizers.L1L2(l1=hp_l1_lr, l2=hp_l2_lr),\n",
        "                        bias_regularizer=tf.keras.regularizers.L2(l2=hp_bias_lr),))\n",
        "# m.add(keras.layers.Dropout(0.2))\n",
        "\n",
        "#Hidden2\n",
        "m.add(keras.layers.Dense(units=98,\n",
        "                        activation=activation,\n",
        "                        kernel_initializer=Init,\n",
        "                        kernel_regularizer=tf.keras.regularizers.L1L2(l1=hp_l1_lr, l2=hp_l2_lr),\n",
        "                        bias_regularizer=tf.keras.regularizers.L2(l2=hp_bias_lr),))\n",
        "# m.add(keras.layers.Dropout(0.4))\n",
        "\n",
        "#Hidden3\n",
        "m.add(keras.layers.Dense(units=65,\n",
        "                        activation=activation,\n",
        "                        kernel_initializer=Init,\n",
        "                        kernel_regularizer=tf.keras.regularizers.L1L2(l1=hp_l1_lr, l2=hp_l2_lr),\n",
        "                        bias_regularizer=tf.keras.regularizers.L2(l2=hp_bias_lr),))\n",
        "# m.add(keras.layers.Dropout(0.2))\n",
        "\n",
        "#Hidden4\n",
        "m.add(keras.layers.Dense(units=26,\n",
        "                        activation=activation,\n",
        "                        kernel_initializer=Init,\n",
        "                        kernel_regularizer=tf.keras.regularizers.L1L2(l1=hp_l1_lr, l2=hp_l2_lr),\n",
        "                        bias_regularizer=tf.keras.regularizers.L2(l2=hp_bias_lr),))\n",
        "# m.add(keras.layers.Dropout(0.2))\n",
        "\n",
        "#Hidden5\n",
        "m.add(keras.layers.Dense(units=111,\n",
        "                        activation=activation,\n",
        "                        kernel_initializer=Init,\n",
        "                        kernel_regularizer=tf.keras.regularizers.L1L2(l1=hp_l1_lr, l2=hp_l2_lr),\n",
        "                        bias_regularizer=tf.keras.regularizers.L2(l2=hp_bias_lr),))\n",
        "# m.add(keras.layers.Dropout(0.2))\n",
        "\n",
        "#Hidden6\n",
        "m.add(keras.layers.Dense(units=91,\n",
        "                        activation=activation,\n",
        "                        kernel_initializer=Init,\n",
        "                        kernel_regularizer=tf.keras.regularizers.L1L2(l1=hp_l1_lr, l2=hp_l2_lr),\n",
        "                        bias_regularizer=tf.keras.regularizers.L2(l2=hp_bias_lr),))\n",
        "# m.add(keras.layers.Dropout(0.2))\n",
        "\n",
        "\n",
        "#Output Layer\n",
        "m.add(keras.layers.Dense(1, kernel_initializer=Init, name='output'))\n",
        "m.compile(optimizer=opt,loss='mse',metrics=['mse'])\n",
        "m.summary()\n",
        "training_complete = TrainingComplete()\n",
        "\n",
        "# # tf. config. list_physical_devices()\n",
        "with tf.device('/device:GPU:0'):\n",
        "  try:\n",
        "    m.fit(tX_transformed, tY, epochs=43, batch_size=50, verbose=10, callbacks=[training_complete]) #callbacks=[learningRateCallBack, training_complete] , validation_data=(sX1_transformed, sY1) reducedLR  , validation_split=0.3\n",
        "  except:\n",
        "    print('Quick ending')\n",
        "\n",
        "# m.fit(tX_transformed, tY, epochs=43, batch_size=50,validation_split=0.3)\n",
        "\n",
        "#EPOCHS 43 is best but take too much time 42 is much faster\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cYnKAFgLh8p",
        "outputId": "4a25e35b-909c-4d18-a023-ad852742e959"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 67)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 111)               7548      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 98)                10976     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 65)                6435      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 26)                1716      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 111)               2997      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 91)                10192     \n",
            "                                                                 \n",
            " output (Dense)              (None, 1)                 92        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 39,956\n",
            "Trainable params: 39,956\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/43\n",
            "Epoch 2/43\n",
            "Epoch 3/43\n",
            "Epoch 4/43\n",
            "Epoch 5/43\n",
            "Epoch 6/43\n",
            "Epoch 7/43\n",
            "Epoch 8/43\n",
            "Epoch 9/43\n",
            "Epoch 10/43\n",
            "Epoch 11/43\n",
            "Epoch 12/43\n",
            "Epoch 13/43\n",
            "Epoch 14/43\n",
            "Epoch 15/43\n",
            "Epoch 16/43\n",
            "Epoch 17/43\n",
            "Epoch 18/43\n",
            "Epoch 19/43\n",
            "Epoch 20/43\n",
            "Epoch 21/43\n",
            "Epoch 22/43\n",
            "Epoch 23/43\n",
            "Epoch 24/43\n",
            "Epoch 25/43\n",
            "Epoch 26/43\n",
            "Epoch 27/43\n",
            "Epoch 28/43\n",
            "Epoch 29/43\n",
            "Epoch 30/43\n",
            "Epoch 31/43\n",
            "Epoch 32/43\n",
            "Epoch 33/43\n",
            "Epoch 34/43\n",
            "Epoch 35/43\n",
            "Epoch 36/43\n",
            "Epoch 37/43\n",
            "Epoch 38/43\n",
            "Epoch 39/43\n",
            "Epoch 40/43\n",
            "Epoch 41/43\n",
            "Epoch 42/43\n",
            "Epoch 43/43\n",
            "Training has ended.\n",
            "Quick ending\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "predictionTest = m.predict(tX_transformed)\n",
        "#clip predictions\n",
        "predictionTest = np.maximum(predictionTest, 0)\n",
        "print(f\"MSE score: {mean_squared_error(tY, predictionTest)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pn79LMF3Ll3x",
        "outputId": "75bc5d4d-162a-48ec-f0c0-baf7d007369c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "197/197 [==============================] - 1s 2ms/step\n",
            "MSE score: 10.074623674867034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import mean_squared_error\n",
        "# predictionTest = m.predict(tX_transformed)\n",
        "# print(f\"MSE score: {mean_squared_error(tY, predictionTest)}\")"
      ],
      "metadata": {
        "id": "CklFQr_dNSCk"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fslr4WDXifWC"
      },
      "source": [
        "The model generates a baseline submission CSV file, see Colab folder (üóÄ on the left), which you candownload and submit to Kaggle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvKBrGilpTPy"
      },
      "source": [
        "<font color=green><h3><b>$\\gamma$. Make predictions</b><h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "6lWV9-5HTnnm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "617de112-c5ef-4234-c497-3e5e806ef584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "197/197 [==============================] - 0s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "#Create predictions and clip lower values to 0\n",
        "pY = pd.DataFrame(np.maximum(m.predict(vX_transformed), 0), index=np.arange(len(vX_transformed))+1, columns=['y'])\n",
        "ToCSV(pY.round(1), '‚ò¢Ô∏èMilosz submission6')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMVDxm-kEJis"
      },
      "source": [
        "<font color=green><h3><b>$\\delta$. Documentation</b></h3></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtOV3RVcwHAD"
      },
      "source": [
        "<font color=green><h4><b>Task 1. Explain Decisions in Preprocessing Pipeline</b></h4></font>\n",
        "\n",
        "<font color=green>\n",
        "Explain elements of your preprocessing pipeline i.e. feature engineering, subsampling, clustering, dimensionality reduction, etc.</font>\n",
        "\n",
        "<font color=green>\n",
        "\n",
        "1. Why did you choose these elements? (Something in EDA, prior experience,...? Note: EDA is not required)\n",
        "1. How do you evaluate the effectiveness of these elements?\n",
        "1. What else have you tried that worked or didn't?\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39zfwMOXwHAD"
      },
      "source": [
        "<font color=red><b>Your answer here.</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppwkSmoEwHAD"
      },
      "source": [
        "<font color=green><h4><b>Task 2. Explain Decisions in Modeling Pipeline</b></h4></font>\n",
        "\n",
        "<font color=green>\n",
        "Explain your modeling approach, i.e. ideas you tried and why you thought they would be helpful.\n",
        "\n",
        "1. How did these decisions guide you in modeling?\n",
        "1. How do you evaluate the effectiveness of these elements?\n",
        "1. What else have you tried that worked or didn't?\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBXlrkB9wHAD"
      },
      "source": [
        "<font color=red><b>Your answer here.</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzBsjCvS_kEw"
      },
      "source": [
        "<font color=green><h3><b>$\\epsilon$. References</b></h3></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kr8Q-9T_nAb"
      },
      "source": [
        "<font color=red><b>Your answer here.</b></font>\n",
        "\n",
        "<font color=green>\n",
        "Cite your sources to help your peers learn from these (and to avoid plagiarism claims). At the least, HOML textbook should be cited. Use Google Scholar to draw APA citation format for books and publications. Also cite StackOverflow, package documentation, and other meaningful internet resources.\n",
        "\n",
        "1. ...\n",
        "1. ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoF2GoB_QGw9"
      },
      "source": [
        "<font size=5>‚åõ</font> <strong><font color=green size=5>Do not exceed competition's runtime limit! Do not write code outside TGP</font></strong>\n",
        "<hr color=green size=40>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rs4wlpyUPxFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e1f821f-22cd-408e-fc17-6020693f0d75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runtime is 49 sec\n"
          ]
        }
      ],
      "source": [
        "tmr.ShowTime()    # measure Colab's runtime. Do not remove. Keep as the last cell in your notebook."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Trial 10 summary\n",
        "# # Hyperparameters:\n",
        "# # l1_lr: 1e-06\n",
        "# # l2_lr: 0.01\n",
        "# # bias_lr: 0.1\n",
        "# # activation: 8\n",
        "# # init: 0\n",
        "# # num_layers: 4\n",
        "# # units_0: 8\n",
        "# # units_1: 93\n",
        "# # units_2: 86\n",
        "# # learning_rate: 0.01\n",
        "# # beta_1: 0.999\n",
        "# # beta_2: 0.9\n",
        "# # optimizer: 5\n",
        "# # units_3: 80\n",
        "# # units_4: 92\n",
        "# # Score: 20.267539978027344\n",
        "\n",
        "\n",
        "# #     c_opt_options = {\n",
        "# #               0: (keras.optimizers.Nadam(learning_rate= hp_learning_rate, beta_1=hp_beta1, beta_2=hp_beta2)),\n",
        "# #               1: (keras.optimizers.SGD(learning_rate=hp_learning_rate)),\n",
        "# #               2: (keras.optimizers.RMSprop(learning_rate=hp_learning_rate)),\n",
        "# #               3: (keras.optimizers.Adagrad(learning_rate=hp_learning_rate)),\n",
        "# #               4: (keras.optimizers.Adadelta(learning_rate=hp_learning_rate)),\n",
        "# #               5: (keras.optimizers.Adam(learning_rate=hp_learning_rate)),\n",
        "# #               6: (keras.optimizers.Adamax(learning_rate=hp_learning_rate))\n",
        "\n",
        "# #    a_act_opts = {\n",
        "# #               0: ('softmax'),\n",
        "# #               1: ('softplus'),\n",
        "# #               2: ('softsign'),\n",
        "# #               3: ('relu'),\n",
        "# #               4: ('tanh'),\n",
        "# #               5: ('sigmoid'),\n",
        "# #               6: ('hard_sigmoid'),\n",
        "# #               7: ('linear'),\n",
        "# #               8: ('elu'),\n",
        "# #               9: ('selu'),\n",
        "# #               10: (tf.keras.layers.LeakyReLU(alpha=0.05))\n",
        "# #             }\n",
        "# #     b_init_options = {\n",
        "# #               0: ('uniform'),\n",
        "# #               1: ('lecun_uniform'),\n",
        "# #               2: ('normal'),\n",
        "# #               3: ('zero'),\n",
        "# #               4: ('glorot_normal'),\n",
        "# #               5: ('glorot_uniform'),\n",
        "# #               6: ('he_normal'),\n",
        "# #               7: ('he_uniform'),\n",
        "# #               8: (keras.initializers.RandomNormal(seed=0))\n",
        "\n",
        "\n",
        "\n",
        "# # Init = keras.initializers.RandomNormal(seed=13)\n",
        "# Init = tf.keras.initializers.HeNormal(seed=13)\n",
        "# activation = 'elu'\n",
        "# hp_learning_rate = 0.01\n",
        "# hp_beta1 = 0.999\n",
        "# hp_beta2 = 0.9\n",
        "# hp_l1_lr=  1e-06\n",
        "# hp_l2_lr= 0.01\n",
        "# hp_bias_lr = 0.1\n",
        "# leakyReluAlpha = 0.05\n",
        "# LRSchedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=hp_learning_rate, decay_steps=10000, decay_rate=0.90)\n",
        "# opt = keras.optimizers.Adam(learning_rate=hp_learning_rate)\n",
        "\n",
        "# m = keras.Sequential()\n",
        "# m.add(keras.layers.Flatten(input_shape=[tX_transformed.shape[1]]))\n",
        "# # m.add(keras.layers.Dropout(0))\n",
        "# # earlyStopping = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True, verbose=0),\n",
        "# # reducedLR = tf.keras.callbacks.ReduceLROnPlateau(patience=3, factor=0.4, verbose=0)\n",
        "\n",
        "# #Hidden1\n",
        "# m.add(keras.layers.Dense(units=30,\n",
        "#                         activation=activation,\n",
        "#                         kernel_initializer=Init,\n",
        "#                         kernel_regularizer=tf.keras.regularizers.L1L2(l1=hp_l1_lr, l2=hp_l2_lr),\n",
        "#                         bias_regularizer=tf.keras.regularizers.L2(l2=hp_bias_lr),))\n",
        "# # m.add(keras.layers.Dropout(0.2))\n",
        "\n",
        "# #Hidden2\n",
        "# m.add(keras.layers.Dense(units=93,\n",
        "#                         activation=activation,\n",
        "#                         kernel_initializer=Init,\n",
        "#                         kernel_regularizer=tf.keras.regularizers.L1L2(l1=hp_l1_lr, l2=hp_l2_lr),\n",
        "#                         bias_regularizer=tf.keras.regularizers.L2(l2=hp_bias_lr),))\n",
        "# # m.add(keras.layers.Dropout(0.4))\n",
        "\n",
        "# #Hidden3\n",
        "# m.add(keras.layers.Dense(units=86,\n",
        "#                         activation=activation,\n",
        "#                         kernel_initializer=Init,\n",
        "#                         kernel_regularizer=tf.keras.regularizers.L1L2(l1=hp_l1_lr, l2=hp_l2_lr),\n",
        "#                         bias_regularizer=tf.keras.regularizers.L2(l2=hp_bias_lr),))\n",
        "# # m.add(keras.layers.Dropout(0.2))\n",
        "\n",
        "# #Hidden4\n",
        "# m.add(keras.layers.Dense(units=80,\n",
        "#                         activation=activation,\n",
        "#                         kernel_initializer=Init,\n",
        "#                         kernel_regularizer=tf.keras.regularizers.L1L2(l1=hp_l1_lr, l2=hp_l2_lr),\n",
        "#                         bias_regularizer=tf.keras.regularizers.L2(l2=hp_bias_lr),))\n",
        "# # m.add(keras.layers.Dropout(0.2))\n",
        "\n",
        "# #Output Layer\n",
        "# m.add(keras.layers.Dense(1, kernel_initializer=Init, name='output'))\n",
        "# m.compile(optimizer=opt,loss='mse',metrics=['mse'])\n",
        "# m.summary()\n",
        "# training_complete = TrainingComplete()\n",
        "\n",
        "# # # tf. config. list_physical_devices()\n",
        "# with tf.device('/device:GPU:0'):\n",
        "#   try:\n",
        "#     m.fit(tX_transformed, tY, epochs=37, batch_size=50, verbose=10, callbacks=[training_complete]) #callbacks=[learningRateCallBack, training_complete] , validation_data=(sX1_transformed, sY1) reducedLR  , validation_split=0.3\n",
        "#   except:\n",
        "#     print('Quick ending')\n",
        "\n",
        "# # m.fit(tX_transformed, tY, epochs=30, batch_size=50,validation_split=0.3)\n"
      ],
      "metadata": {
        "id": "Yl7lpj12lX7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import mean_squared_error\n",
        "# predictionTest = m.predict(tX_transformed)\n",
        "# print(f\"MSE score: {mean_squared_error(tY, predictionTest)}\")"
      ],
      "metadata": {
        "id": "JHxFPSaZsMbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgH9_HiGk6uq"
      },
      "source": [
        "# **Starter Ideas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K732flZJlKNA"
      },
      "source": [
        "1. Try different DNN architectures and tuning of hyperparameters\n",
        "1. Try converting locations to distances to the key Radon sources (which you might need to discover).\n",
        "1. Try clustering categorical variables by their relation to Radon levels\n",
        "1. Try replacing categorical values with their level frequencies or other encodings\n",
        "1. Try scaling features linearly or nonlinearly\n",
        "1. Try embedding **textual** values (eg. US States names) with pre-trained SBERT-like models. This injects some additional information from Wikipedia (or whichever corpora were used for model training).\n",
        "1. Do EDA and understand the variables and their relation to the output. [Example 1](https://docs.pymc.io/en/v3.11.4/pymc-examples/examples/case_studies/multilevel_modeling.html), [Example 2](https://www.tensorflow.org/probability/examples/Multilevel_Modeling_Primer)\n",
        "\n",
        "<hr>\n",
        "<font color=black>\n",
        "    <details><summary><font color=carnelian>‚ñ∂ </font>Clustering categorical variables <b></b>.</summary>\n",
        "\n",
        "  1. When we represent categorical variables as dummies, we may be losing important multivariate information. For example, say we use weekdays to predict the number of hours a person works. We could convert weekdays to 6 features (one is dropped due to collinearity). This requires 6 coefficients (degrees of freedom or sources of uncertainty). Essentially, we have an overparameterized model, whereas all we really need is two clusters of categorical values - weekends (Sat/Sun) and non-weekends (M/T/W/Th/F). In general, the model overparameterized model will do worse due to higher variance of the model output (resulting from the overfit and higher flexibility).\n",
        "\n",
        "  1. Here is another example from the NLP domain, where each word is a feature (or dimension). While morphological variants of a word (eg. run, running, runner, ran, runs, ...) have lower frequency, we cluster them into the same lemma \"run\", assuming only a small loss of semantic information. We hope that the gain in building a better distribution estimate for \"run\" is greater than the loss of semantic and lexical information.\n",
        "        </details>\n",
        "    <details><summary><font color=carnelian>‚ñ∂ </font>Distance to Radon source<b></b>.</summary>\n",
        "\n",
        "If you can determine where Radon is most active (i.e. the source), then you might be able to compute the distance to the source. Ordinarily, we expect lower radiation for greater distance from the source (assuming uniform distribution of underground rivers, geology, rains/winds and other weather conditions affecting distribution of radon, etc.). You could also use categorical features in (e.g. US State, region, etc.), but these might perform better when clustered (again). Distance to the source is a real-valued feature, which does not require clustering.\n",
        "        </details>\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjPZ1w_JdPea"
      },
      "outputs": [],
      "source": [
        "# tf.random.set_seed(0)   # always seed your experiments\n",
        "# Init = keras.initializers.RandomNormal(seed=0)\n",
        "\n",
        "# m = keras.models.Sequential([\n",
        "#   Flatten(input_shape=[tX.shape[1]]),\n",
        "#   Dense(5, activation=\"relu\", kernel_initializer=Init),\n",
        "#   Dense(5, activation=\"relu\", kernel_initializer=Init),\n",
        "#   Dense(5, activation=\"relu\", kernel_initializer=Init),\n",
        "#   Dense(1, kernel_initializer=Init)])\n",
        "# m.summary()\n",
        "# m.compile(loss=\"mse\", optimizer=\"adam\", metrics=['mse'])\n",
        "# hist = m.fit(x=tX, y=tY, batch_size=32, epochs=5, validation_split=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model: \"sequential\"\n",
        "# _________________________________________________________________\n",
        "#  Layer (type)                Output Shape              Param #\n",
        "# =================================================================\n",
        "#  flatten (Flatten)           (None, 67)                0\n",
        "\n",
        "#  dense (Dense)               (None, 4)                 272\n",
        "\n",
        "#  dense_1 (Dense)             (None, 86)                430\n",
        "\n",
        "#  dense_2 (Dense)             (None, 87)                7569\n",
        "\n",
        "#  output (Dense)              (None, 1)                 88\n",
        "\n",
        "# =================================================================\n",
        "# Total params: 8,359\n",
        "# Trainable params: 8,359\n",
        "# Non-trainable params: 0\n",
        "# _________________________________________________________________\n",
        "\n",
        "\n",
        "\n",
        "# Trial 11 summary\n",
        "# Hyperparameters:\n",
        "# l1_lr: 1e-05\n",
        "# l2_lr: 0.01\n",
        "# bias_lr: 1e-05\n",
        "# activation: 8\n",
        "# init: 1\n",
        "# num_layers: 3\n",
        "# units_0: 4\n",
        "# units_1: 86\n",
        "# units_2: 87\n",
        "# learning_rate: 0.05\n",
        "# beta_1: 0.9\n",
        "# beta_2: 0.9\n",
        "# optimizer: 2\n",
        "# units_3: 45\n",
        "# units_4: 23\n",
        "# Score: 17.085132598876953\n",
        "\n",
        "# Trial 10 summary\n",
        "# Hyperparameters:\n",
        "# l1_lr: 1e-06\n",
        "# l2_lr: 0.01\n",
        "# bias_lr: 0.1\n",
        "# activation: 8\n",
        "# init: 0\n",
        "# num_layers: 4\n",
        "# units_0: 8\n",
        "# units_1: 93\n",
        "# units_2: 86\n",
        "# learning_rate: 0.01\n",
        "# beta_1: 0.999\n",
        "# beta_2: 0.9\n",
        "# optimizer: 5\n",
        "# units_3: 80\n",
        "# units_4: 92\n",
        "# Score: 20.267539978027344\n",
        "\n",
        "# Trial 13 summary\n",
        "# Hyperparameters:\n",
        "# l1_lr: 0.0001\n",
        "# l2_lr: 0.001\n",
        "# bias_lr: 0.0001\n",
        "# activation: 5\n",
        "# init: 1\n",
        "# num_layers: 3\n",
        "# units_0: 84\n",
        "# units_1: 93\n",
        "# units_2: 35\n",
        "# learning_rate: 0.1\n",
        "# beta_1: 0.99\n",
        "# beta_2: 0.9\n",
        "# optimizer: 1\n",
        "# units_3: 10\n",
        "# units_4: 37\n",
        "# Score: 32.629981994628906\n",
        "\n",
        "# Trial 04 summary\n",
        "# Hyperparameters:\n",
        "# l1_lr: 1e-05\n",
        "# l2_lr: 0.0\n",
        "# bias_lr: 0.0001\n",
        "# activation: 3\n",
        "# init: 0\n",
        "# num_layers: 3\n",
        "# units_0: 88\n",
        "# units_1: 75\n",
        "# units_2: 43\n",
        "# learning_rate: 0.1\n",
        "# beta_1: 0.99\n",
        "# beta_2: 0.7\n",
        "# optimizer: 3\n",
        "# units_3: 98\n",
        "# units_4: 8\n",
        "# Score: 33.90720748901367\n",
        "\n"
      ],
      "metadata": {
        "id": "Dp4bH0z65j7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# _________________________________________________________________\n",
        "#  Layer (type)                Output Shape              Param #\n",
        "# =================================================================\n",
        "#  flatten (Flatten)           (None, 67)                0\n",
        "\n",
        "#  dense (Dense)               (None, 111)               7548\n",
        "\n",
        "#  dense_1 (Dense)             (None, 98)                10976\n",
        "\n",
        "#  dense_2 (Dense)             (None, 65)                6435\n",
        "\n",
        "#  dense_3 (Dense)             (None, 26)                1716\n",
        "\n",
        "#  dense_4 (Dense)             (None, 111)               2997\n",
        "\n",
        "#  dense_5 (Dense)             (None, 91)                10192\n",
        "\n",
        "#  output (Dense)              (None, 1)                 92\n",
        "\n",
        "# =================================================================\n",
        "# Total params: 39,956\n",
        "# Trainable params: 39,956\n",
        "# Non-trainable params: 0\n",
        "# _________________________________________________________________\n",
        "\n",
        "# Results summary\n",
        "# Results in ./trial1\n",
        "# Showing 10 best trials\n",
        "# Objective(name=\"val_mse\", direction=\"min\")\n",
        "\n",
        "# Trial 20 summary\n",
        "# Hyperparameters:\n",
        "# l1_lr: 1e-05\n",
        "# l2_lr: 1e-05\n",
        "# bias_lr: 0.1\n",
        "# activation: 2\n",
        "# init: 0\n",
        "# num_layers: 6\n",
        "# units_0: 111\n",
        "# units_1: 98\n",
        "# units_2: 65\n",
        "# learning_rate: 0.05\n",
        "# beta_1: 0.7\n",
        "# beta_2: 0.99\n",
        "# optimizer: 6\n",
        "# units_3: 26\n",
        "# units_4: 111\n",
        "# units_5: 91\n",
        "# Score: 20.706668853759766\n",
        "\n",
        "# Trial 12 summary\n",
        "# Hyperparameters:\n",
        "# l1_lr: 1e-06\n",
        "# l2_lr: 0.0001\n",
        "# bias_lr: 0.001\n",
        "# activation: 10\n",
        "# init: 1\n",
        "# num_layers: 4\n",
        "# units_0: 39\n",
        "# units_1: 62\n",
        "# units_2: 102\n",
        "# learning_rate: 0.05\n",
        "# beta_1: 0.7\n",
        "# beta_2: 0.7\n",
        "# optimizer: 0\n",
        "# units_3: 51\n",
        "# units_4: 83\n",
        "# Score: 25.305789947509766\n",
        "\n",
        "# Trial 11 summary\n",
        "# Hyperparameters:\n",
        "# l1_lr: 0.001\n",
        "# l2_lr: 0.0\n",
        "# bias_lr: 0.001\n",
        "# activation: 1\n",
        "# init: 1\n",
        "# num_layers: 3\n",
        "# units_0: 11\n",
        "# units_1: 89\n",
        "# units_2: 115\n",
        "# learning_rate: 0.01\n",
        "# beta_1: 0.7\n",
        "# beta_2: 0.7\n",
        "# optimizer: 0\n",
        "# units_3: 46\n",
        "# units_4: 47\n",
        "# Score: 26.025346755981445\n",
        "\n",
        "# Trial 07 summary\n",
        "# Hyperparameters:\n",
        "# l1_lr: 0.0001\n",
        "# l2_lr: 0.001\n",
        "# bias_lr: 0.001\n",
        "# activation: 1\n",
        "# init: 0\n",
        "# num_layers: 4\n",
        "# units_0: 41\n",
        "# units_1: 115\n",
        "# units_2: 72\n",
        "# learning_rate: 0.02\n",
        "# beta_1: 0.99\n",
        "# beta_2: 0.99\n",
        "# optimizer: 6\n",
        "# units_3: 95\n",
        "# units_4: 38\n",
        "# Score: 27.15325927734375"
      ],
      "metadata": {
        "id": "Rncm4TKbCuqh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}